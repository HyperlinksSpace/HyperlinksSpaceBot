# AI backend env template — local defaults: Ollama on, OpenAI off
# Required
INNER_CALLS_KEY=change-me-shared-secret

# RAG wiring (set to deployed/local RAG base URL)
RAG_URL=http://127.0.0.1:8001

# LLM switches — LOCAL: leave as below so Ollama is used (do not set OPENAI_SWITCH=1 or OPENAI_KEY)
OLLAMA_SWITCH=1
OPENAI_SWITCH=0

# Ollama (used when OLLAMA_SWITCH=1; primary or fallback after OpenAI)
OLLAMA_URL=http://127.0.0.1:11434
OLLAMA_MODEL=qwen2.5:0.5b-instruct

# OpenAI — for LOCAL Ollama-only: do NOT set OPENAI_KEY or OPENAI_SWITCH=1 here
# (used when OPENAI_SWITCH=1; OPENAI_KEY required). Default model gpt-4o.
# OPENAI_KEY=sk-...
# OPENAI_MODEL=gpt-4o

# Optional: Cocoon client (when both switches off or as fallback)
# COCOON_CLIENT_URL=http://127.0.0.1:10000
# COCOON_MODEL=default

# Optional compatibility alias still accepted:
# API_KEY=change-me-shared-secret
