# AI backend env template â€” local defaults: Ollama on, OpenAI off
# Required
INNER_CALLS_KEY=change-me-shared-secret

# RAG wiring (set to deployed/local RAG base URL)
RAG_URL=http://127.0.0.1:8001

# LLM switches: locally use defaults below (Ollama deploys/start + model pull; OpenAI off)
OLLAMA_SWITCH=1
OPENAI_SWITCH=0

# Ollama (used when OLLAMA_SWITCH=1; primary or fallback after OpenAI)
OLLAMA_URL=http://127.0.0.1:11434
OLLAMA_MODEL=qwen2.5:0.5b-instruct

# OpenAI (used when OPENAI_SWITCH=1; OPENAI_KEY required). Default model gpt-4o.
# OPENAI_KEY=sk-...
# OPENAI_MODEL=gpt-4o

# Optional: Cocoon client (when both switches off or as fallback)
# COCOON_CLIENT_URL=http://127.0.0.1:10000
# COCOON_MODEL=default

# Optional compatibility alias still accepted:
# API_KEY=change-me-shared-secret
